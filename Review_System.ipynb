{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c19c98fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from string import punctuation\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from numba import cuda\n",
    "device=torch.device(\"cuda:0\")\n",
    "train_on_gpu=True #To use GPU make this variable True\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb01349f",
   "metadata": {},
   "source": [
    "# Data-preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c34b7b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/reviews.txt') as f:\n",
    "    reviews=f.read()\n",
    "with open('Data/labels.txt') as t:\n",
    "    labels=t.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1777cc40",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reviews=reviews.lower()\n",
    "all_text=''.join([c for c in reviews if c not in punctuation])\n",
    "reviews_split=all_text.split('\\n')\n",
    "words=all_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21db64b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words=set(words)\n",
    "wordsTOint={word:token+1 for token,word in enumerate(unique_words) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32c90c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_int=[[]]*len(reviews_split)\n",
    "for i in range(len(reviews_split)):\n",
    "    reviews_int[i]=reviews_split[i].split()\n",
    "    reviews_int[i]=[wordsTOint[j] for j in reviews_int[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af43e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_split=labels.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af74e464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 for positive ,0 for negative\n",
    "labels_int=[0]*len(labels_split)\n",
    "j=0\n",
    "for i in labels_split:\n",
    "    if(i=='positive'):\n",
    "        labels_int[j]=1\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "124cd78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing 0 length text\n",
    "count=0\n",
    "idx=0\n",
    "for i in range(len(reviews_int)):\n",
    "    if(len(reviews_int[i])==0):\n",
    "        count+=1\n",
    "        idx=i  \n",
    "\n",
    "del reviews_int[idx]\n",
    "del labels_int[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61980b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding and truncation to 250 words\n",
    "reviews_inp=[[]]*len(reviews_int)\n",
    "for i in range(len(reviews_int)):\n",
    "    if(len(reviews_int[i])>=250):\n",
    "        reviews_inp[i]=[reviews_int[i][j] for j in range(250)]\n",
    "    else:\n",
    "        reviews_inp[i]=[reviews_int[i][j] for j in range(len(reviews_int[i]))]\n",
    "        reviews_inp[i]+=[0]*(250-len(reviews_int[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "229e4c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating training testing and validation dataset of train size 80 percent\n",
    "size_train=int(len(reviews_inp)*0.8)\n",
    "train_x,remaining_x=reviews_inp[:size_train],reviews_inp[size_train:len(reviews_inp)]\n",
    "train_y,remaining_y=labels_int[:size_train],labels_int[size_train:len(reviews_inp)]\n",
    "\n",
    "size_test=int(len(remaining_x)*0.5)\n",
    "test_x,val_x=reviews_inp[:size_test],reviews_inp[size_test:len(remaining_x)]\n",
    "test_y,val_y=labels_int[:size_test],labels_int[size_test:len(remaining_x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fe4b8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batching using Dataloader\n",
    "train_data=TensorDataset(torch.Tensor(train_x),torch.Tensor(train_y))\n",
    "test_data=TensorDataset(torch.Tensor(test_x),torch.Tensor(test_y))\n",
    "val_data=TensorDataset(torch.Tensor(val_x),torch.Tensor(val_y))\n",
    "\n",
    "batch_size=50\n",
    "train_loader=DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
    "test_loader=DataLoader(test_data,batch_size=batch_size)\n",
    "val_loader=DataLoader(val_data,batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91d9eb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 250]) torch.Size([50])\n",
      "tensor([[20742.,  5747.,  3181.,  ...,     0.,     0.,     0.],\n",
      "        [22608., 10591., 24425.,  ...,     0.,     0.,     0.],\n",
      "        [60595., 24425., 32252.,  ...,     0.,     0.,     0.],\n",
      "        ...,\n",
      "        [20742., 27207., 23826.,  ...,     0.,     0.,     0.],\n",
      "        [ 5796., 24368., 21501.,  ..., 24425.,  8500., 69416.],\n",
      "        [26358., 69505., 18028.,  ...,     0.,     0.,     0.]])\n"
     ]
    }
   ],
   "source": [
    "#output is [batch_size,seq len]\n",
    "dataiter=iter(train_loader)\n",
    "x,y=next(dataiter)\n",
    "print(x.size(),y.size())\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e36bf0d",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61294974",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRnn(nn.Module):\n",
    "    def __init__(self,vocab_size,emb_size,hidden_size,output_size,n_layers,drop_prob):\n",
    "        super().__init__()\n",
    "        self.vocab_size=vocab_size\n",
    "        self.emb_size=emb_size\n",
    "        self.hidden_size=hidden_size #hidden_size or memory_size\n",
    "        self.output_size=output_size\n",
    "        self.n_layers=n_layers\n",
    "        self.drop_prob=drop_prob\n",
    "        \n",
    "        self.embedding=nn.Embedding(self.vocab_size,self.emb_size)\n",
    "        self.lstm=nn.LSTM(self.emb_size,self.hidden_size,num_layers=self.n_layers,batch_first=True,dropout=self.drop_prob)\n",
    "        \n",
    "        self.L1=nn.Linear(self.hidden_size,self.output_size)\n",
    "        self.sig=nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x,memory):\n",
    "        batch_size=x.size(0)\n",
    "        x=x.long()\n",
    "        x=self.embedding(x)\n",
    "        x,memory=self.lstm(x,memory)\n",
    "        x=x.contiguous().view(-1,self.hidden_size)\n",
    "        x=self.sig(self.L1(x))\n",
    "        x=x.view(batch_size,-1)\n",
    "        x=x[:,-1]\n",
    "        return x,memory\n",
    "    \n",
    "    def init_hidden(self,batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            memory=(weight.new(self.n_layers,batch_size,self.hidden_size).zero_().cuda(),\n",
    "                    weight.new(self.n_layers,batch_size,self.hidden_size).zero_().cuda())\n",
    "        else:\n",
    "            memory=(weight.new(self.n_layers,batch_size,self.hidden_size).zero_(),\n",
    "                    weight.new(self.n_layers,batch_size,self.hidden_size).zero_())\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8a4329",
   "metadata": {},
   "source": [
    "# Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f274e50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRnn(\n",
      "  (embedding): Embedding(74073, 500)\n",
      "  (lstm): LSTM(500, 250, num_layers=2, batch_first=True, dropout=0.25)\n",
      "  (L1): Linear(in_features=250, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size=len(unique_words)+1 # +1 for padding token\n",
    "emb_size=500\n",
    "hidden_size=250\n",
    "output_size=1\n",
    "n_layers=2\n",
    "drop_prob=0.25\n",
    "model=SentimentRnn(vocab_size,emb_size,hidden_size,output_size,n_layers,drop_prob)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283e1d77",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e653e286",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [01:43<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20 loss:0.6972181797027588 val_loss:0.6889009487628937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [02:03<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20 loss:0.6628994345664978 val_loss:0.682300478219986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [02:48<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20 loss:0.42085981369018555 val_loss:0.3356804347038269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [02:46<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20 loss:0.27438297867774963 val_loss:0.23062832683324813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [03:00<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20 loss:0.2518778443336487 val_loss:0.15734880149364472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [03:07<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20 loss:0.05890902504324913 val_loss:0.08432800490409136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [03:25<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20 loss:0.13598604500293732 val_loss:0.05255762210115791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [03:18<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20 loss:0.020424121990799904 val_loss:0.025153682632371784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [03:18<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20 loss:0.17024913430213928 val_loss:0.04317868530750275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [03:45<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20 loss:0.04330524429678917 val_loss:0.020837718909606336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [03:52<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20 loss:0.006044161971658468 val_loss:0.013915030073840172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [04:35<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20 loss:0.24662530422210693 val_loss:0.010781166146043688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [04:55<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20 loss:0.02429526299238205 val_loss:0.015440317369066179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [05:29<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20 loss:0.0032402127981185913 val_loss:0.011880716888699681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [05:44<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20 loss:0.0028072355780750513 val_loss:0.011756487370003016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [05:46<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20 loss:0.0052403113804757595 val_loss:0.03468785788398236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [05:51<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20 loss:0.020875845104455948 val_loss:0.0069986439589411024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [05:15<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20 loss:0.008807538077235222 val_loss:0.008414144580019638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [05:47<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20 loss:0.009807484224438667 val_loss:0.005250590714276768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [06:04<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20 loss:0.002642920007929206 val_loss:0.004757403862895444\n"
     ]
    }
   ],
   "source": [
    "if(train_on_gpu):\n",
    "    model.to(device)\n",
    "epochs=20\n",
    "clip=5 # gradient clipping\n",
    "lr=0.001\n",
    "criterion=nn.BCELoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=lr)\n",
    "epoch=1\n",
    "for i in range(epochs):\n",
    "    model.train()\n",
    "    torch.cuda.empty_cache()\n",
    "    memory=model.init_hidden(batch_size)\n",
    "    for inputs,labels in tqdm(train_loader):\n",
    "        if(train_on_gpu):\n",
    "            inputs,labels=inputs.to(device),labels.to(device)\n",
    "        pred,memory=model(inputs,memory)\n",
    "        memory=tuple([each.data for each in memory])\n",
    "        loss=criterion(pred,labels.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    torch.cuda.empty_cache()\n",
    "    val_h=model.init_hidden(batch_size)\n",
    "    val_losses=[]\n",
    "    for inputs,labels in val_loader:\n",
    "        if(train_on_gpu):\n",
    "            inputs,labels=inputs.to(device),labels.to(device)\n",
    "        output,val_h=model(inputs,val_h)\n",
    "        val_h=tuple([each.data for each in val_h])\n",
    "        val_loss=criterion(output,labels.float())\n",
    "        val_losses.append(val_loss.item())\n",
    "    \n",
    "    print(f'Epoch:{epoch}',f'loss:{loss.item()}',f'val_loss:{np.mean(val_losses)}')\n",
    "    epoch+=1   \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c348f351",
   "metadata": {},
   "source": [
    "# Save Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b74a259",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'rnn_20_epoch.net'\n",
    "\n",
    "checkpoint = {'vocab_size': vocab_size,\n",
    "              'emb_size': emb_size,\n",
    "              'hidden_size': hidden_size,\n",
    "              'output_size': output_size,\n",
    "              'n_layers': n_layers,\n",
    "              'drop_prob': drop_prob,\n",
    "              'state_dict': model.state_dict(),\n",
    "               'wordTOint':wordsTOint}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a9048b",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a14b56b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracy: 0.9988\n"
     ]
    }
   ],
   "source": [
    "if(train_on_gpu):\n",
    "    model.to(device)\n",
    "batch_size=50\n",
    "model.eval()\n",
    "torch.cuda.empty_cache()\n",
    "test_h=model.init_hidden(batch_size)\n",
    "correct_test=0\n",
    "for inputs,labels in test_loader:\n",
    "    if(train_on_gpu):\n",
    "        inputs,labels=inputs.to(device),labels.to(device)\n",
    "    output,test_h=model(inputs,test_h)\n",
    "    test_h=tuple([each.data for each in test_h])\n",
    "    output=torch.round(output).detach().cpu().numpy()\n",
    "    labels=labels.cpu().numpy()\n",
    "    for i in range(len(labels)):\n",
    "        if(labels[i]==output[i]):\n",
    "            correct_test+=1\n",
    "print(f'test_accuracy: {correct_test/len(test_loader.dataset)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbfe0ad",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "466852dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('rnn_20_epoch.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "vocab_size=checkpoint['vocab_size']\n",
    "emb_size=checkpoint['emb_size']\n",
    "hidden_size=checkpoint['hidden_size']\n",
    "output_size=checkpoint['output_size']\n",
    "n_layers=checkpoint['n_layers']\n",
    "drop_prob=checkpoint['drop_prob']\n",
    "wordTOint=checkpoint['wordTOint']\n",
    "model= SentimentRnn(vocab_size,emb_size,hidden_size,output_size,n_layers,drop_prob)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920fd04c",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e4c8875",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pred:\n",
    "    def __init__(self):\n",
    "        with open('rnn_20_epoch.net', 'rb') as f:\n",
    "            checkpoint = torch.load(f)\n",
    "        self.vocab_size=checkpoint['vocab_size']\n",
    "        self.emb_size=checkpoint['emb_size']\n",
    "        self.hidden_size=checkpoint['hidden_size']\n",
    "        self.output_size=checkpoint['output_size']\n",
    "        self.n_layers=checkpoint['n_layers']\n",
    "        self.drop_prob=checkpoint['drop_prob']\n",
    "        self.wordTOint=checkpoint['wordTOint']\n",
    "        self.loaded = SentimentRnn(self.vocab_size,self.emb_size,self.hidden_size,self.output_size,self.n_layers,self.drop_prob)\n",
    "        self.loaded.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "        \n",
    "    def predict(self,text):\n",
    "        batch_size=1;\n",
    "        reviews=text.lower()\n",
    "        all_text=''.join([c for c in reviews if c not in punctuation])\n",
    "        reviews=all_text.split()\n",
    "        reviews_int=[self.wordTOint[i] for i in reviews]\n",
    "        if(len(reviews_int)>=250):\n",
    "            reviews_inp=reviews_int[:250]\n",
    "        else:\n",
    "            reviews_inp=reviews_int+[0]*(250-len(reviews_int))\n",
    "        inputs=torch.Tensor(reviews_inp)\n",
    "        inputs=inputs.unsqueeze(0)\n",
    "        if(train_on_gpu):\n",
    "            inputs.to(device)\n",
    "        self.loaded.eval()\n",
    "        h=self.loaded.init_hidden(batch_size)\n",
    "        output,h=self.loaded(inputs,h)\n",
    "        output=torch.round(output)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "178bbffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE\n"
     ]
    }
   ],
   "source": [
    "a=pred()\n",
    "b=a.predict('This movie had the best acting and the dialogue was so good. I loved it.')\n",
    "if(b==0):\n",
    "    print(\"NEGATIVE\")\n",
    "else:\n",
    "    print(\"POSITIVE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
